{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "from collections import Counter\n",
    "import spacy\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from datetime import datetime"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "endpoint variable has been configured to: https://sciencedata.dk/files/\n"
     ]
    }
   ],
   "source": [
    "# to communicate with google spreadsheet...\n",
    "import gspread\n",
    "from gspread_dataframe import get_as_dataframe\n",
    "from gspread_dataframe import set_with_dataframe\n",
    "from google.oauth2 import service_account # based on google-auth library\n",
    "import sddk\n",
    "\n",
    "s = sddk.cloudSession(\"sciencedata.dk\")\n",
    "# establish connection with gogglesheets...\n",
    "file_data = s.read_file(\"https://sciencedata.dk/files/ServiceAccountsKey.json\", \"dict\") # or load it from a local storage: json.load(open(\"../../ServiceAccountsKey.json\", \"r\"))\n",
    "credentials = service_account.Credentials.from_service_account_info(file_data)\n",
    "gc = gspread.Client(auth=credentials.with_scopes(['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']))\n",
    "mops_data = gc.open_by_url(\"https://docs.google.com/spreadsheets/d/1VbCIAJssHKV9hlRTwzVFfm40CGnHesq53KXjv2qy4OM/edit?usp=sharing\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "nlp_light = spacy.load('en_core_web_sm')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "postags = [\"PROPN\", \"NOUN\", \"VERB\", \"ADJ\"]\n",
    "\n",
    "def clean_token(token):\n",
    "    token = re.sub(\"\\W\", \"\", token)\n",
    "    if len(token) > 1:\n",
    "        token = token[0] + token[1:].lower()\n",
    "    return token\n",
    "\n",
    "def doc_to_lemmata(doc, postags):\n",
    "    lemmata_list = [t.lemma_ for t in doc if t.pos_ in postags]\n",
    "    lemmata_list = [clean_token(el) for el in lemmata_list if len(el)>0]\n",
    "    joined_lemmata_sorted = \" \".join(sorted(lemmata_list))\n",
    "    return joined_lemmata_sorted\n",
    "\n",
    "def ngram_to_lemmata(ngram, ngram_nlp_dict):\n",
    "    try:\n",
    "        lemmata_sorted_str = ngram_nlp_dict[ngram][\"lemmata\"]\n",
    "        #print(\"found in preprocessed\") # (used for execution time tests...)\n",
    "    except:\n",
    "        try:\n",
    "            lemmata_sorted_str = doc_to_lemmata(nlp(ngram), postags)\n",
    "            #print(\"processed now\") # (used for execution time tests...)\n",
    "        except:\n",
    "            lemmata_sorted_str = \"\"\n",
    "    return lemmata_sorted_str\n",
    "\n",
    "def article_data_to_lemmata(ngrams_dict, ngram_nlp_dict):\n",
    "    lemmata_tups = []\n",
    "    for string, count in ngrams_dict.items():\n",
    "        lemmata_tups.append((ngram_to_lemmata(string, ngram_nlp_dict), count))\n",
    "    lemmata_dict = Counter()\n",
    "    for x,y in lemmata_tups:\n",
    "        lemmata_dict.update({x : y})\n",
    "    return lemmata_dict\n",
    "\n",
    "def data_from_article_id(article_id):\n",
    "    article_data = article_data_to_lemmata(ngramCount_dict[article_id], ngram_nlp_dict)\n",
    "    return (article_id, article_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "ngram_type = \"unigram\"\n",
    "ngramCount_dict = pickle.load(open(\"../data/large_files/{0}Count_dict.pickle\".format(ngram_type), \"rb\"))\n",
    "ngram_nlp_dict = pickle.load(open(\"../data/large_files/data_{0}s_nlp_dict.pickle\".format(ngram_type), \"rb\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 520 ms, sys: 7.57 ms, total: 527 ms\n",
      "Wall time: 551 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": "[('', 1460),\n ('have', 32),\n ('water', 26),\n ('cold', 21),\n ('hot', 20),\n ('city', 11),\n ('Paul', 10),\n ('church', 9),\n ('interpretation', 9),\n ('be', 9),\n ('word', 9),\n ('use', 9),\n ('lukewarm', 8),\n ('local', 8),\n ('Laodicea', 8),\n ('apply', 7),\n ('man', 7),\n ('mean', 7),\n ('spring', 7),\n ('allusion', 7),\n ('other', 6),\n ('certainty', 6),\n ('Christ', 6),\n ('Ramsay', 5),\n ('zesto', 5),\n ('mile', 5),\n ('letter', 5),\n ('fact', 5),\n ('see', 5),\n ('laodicean', 5),\n ('source', 5),\n ('person', 5),\n ('example', 5),\n ('do', 4),\n ('most', 4),\n ('christian', 4),\n ('passage', 4),\n ('supply', 4),\n ('heal', 4),\n ('M', 4),\n ('mineral', 4),\n ('certain', 4),\n ('usage', 4),\n ('purpose', 4),\n ('great', 4),\n ('circumstance', 3),\n ('know', 3),\n ('natural', 3),\n ('Ma', 3),\n ('course', 3),\n ('lukewarmness', 3),\n ('call', 3),\n ('hierapoli', 3),\n ('such', 3),\n ('spiritual', 3),\n ('stone', 3),\n ('sense', 3),\n ('normal', 3),\n ('form', 3),\n ('become', 3),\n ('text', 3),\n ('difficult', 3),\n ('common', 3),\n ('take', 3),\n ('traditional', 3),\n ('site', 3),\n ('neighbour', 3),\n ('difficulty', 3),\n ('same', 3),\n ('suggest', 3),\n ('term', 3),\n ('different', 3),\n ('white', 3),\n ('expect', 3),\n ('part', 3),\n ('exegetical', 3),\n ('imagery', 3),\n ('refer', 3),\n ('point', 3),\n ('greek', 3),\n ('refreshment', 3),\n ('much', 3),\n ('commendable', 3),\n ('kappaalphaiacgr', 3),\n ('read', 3),\n ('argue', 3),\n ('psychro', 3),\n ('condition', 3),\n ('door', 2),\n ('ineffective', 2),\n ('esse', 2),\n ('Romans', 2),\n ('ineffectiveness', 2),\n ('Denizli', 2),\n ('alternative', 2),\n ('derive', 2),\n ('iv', 2),\n ('weary', 2),\n ('series', 2),\n ('concerned', 2)]"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "test_ngrams_dict = (list(ngramCount_dict.items())[7000][1])\n",
    "test_output = article_data_to_lemmata(test_ngrams_dict, ngram_nlp_dict)\n",
    "sorted(test_output.items(), key=lambda pair: pair[1], reverse=True)[:100]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# comparing iteration vs comprehension"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 14s, sys: 2.95 s, total: 2min 17s\n",
      "Wall time: 2min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n = 100\n",
    "# small test...\n",
    "cleanedNgrams_tups = [(k, article_data_to_lemmata(v, ngram_nlp_dict)) for k, v in list(ngramCount_dict.items())[:n]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 9s, sys: 1.12 s, total: 2min 10s\n",
      "Wall time: 2min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# small test...\n",
    "cleanedNgrams_tups = [(k, article_data_to_lemmata(v, ngram_nlp_dict)) for k, v in list(ngramCount_dict.items())[:100]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "# using parallel computing..."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "'ark://27927/phx66812gq6'"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_ids =  list(ngramCount_dict.keys())\n",
    "article_ids[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "14103"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(article_ids)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 28s, sys: 1min, total: 3min 28s\n",
      "Wall time: 1min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "step=20\n",
    "cleanedNgrams_tups = []\n",
    "for num in range(0, 100, step):\n",
    "    actual_ids = article_ids[num:num+step]\n",
    "    with ThreadPoolExecutor(max_workers=step*1.5) as pool:\n",
    "        currently_parsed = list(pool.map(data_from_article_id,actual_ids))\n",
    "    cleanedNgrams_tups.extend(currently_parsed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Main application: unigrams"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10h 45min 58s, sys: 3h 53min 16s, total: 14h 39min 15s\n",
      "Wall time: 8h 35min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ngram_type = \"unigram\"\n",
    "ngramCount_dict = pickle.load(open(\"../data/large_files/{0}Count_dict.pickle\".format(ngram_type), \"rb\"))\n",
    "ngram_nlp_dict = pickle.load(open(\"../data/large_files/data_{0}s_nlp_dict.pickle\".format(ngram_type), \"rb\"))\n",
    "\n",
    "step=50\n",
    "cleanedNgrams_tups = []\n",
    "for num in range(0, len(article_ids), step):\n",
    "    actual_ids = article_ids[num:num+step]\n",
    "    with ThreadPoolExecutor(max_workers=step*1.5) as pool:\n",
    "        currently_parsed = list(pool.map(data_from_article_id,actual_ids))\n",
    "    cleanedNgrams_tups.extend(currently_parsed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "with open(\"../data/large_files/{0}Count_cleaned_dict.pickle\".format(ngram_type), \"wb\") as f:\n",
    "    pickle.dump(dict(cleanedNgrams_tups), f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Main application: trigrams"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 46s, sys: 23.5 s, total: 2min 9s\n",
      "Wall time: 2min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ngram_type = \"trigram\"\n",
    "ngramCount_dict = pickle.load(open(\"../data/large_files/{0}Count_dict.pickle\".format(ngram_type), \"rb\"))\n",
    "ngram_nlp_dict = pickle.load(open(\"../data/large_files/data_{0}s_nlp_dict.pickle\".format(ngram_type), \"rb\"))\n",
    "ngram_nlp_dict2 = pickle.load(open(\"../data/large_files/data_trigrams2_nlp_dict.pickle\", \"rb\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "458444"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ngram_nlp_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "342693"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ngram_nlp_dict2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "ngram_nlp_dict.update(ngram_nlp_dict2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "801137"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ngram_nlp_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "ngram_nlp_tups = []\n",
    "for k, v in list(ngram_nlp_dict.items()):\n",
    "    v.update({\"lemmata\" : doc_to_lemmata(v[\"doc\"], postags)})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "[('. . .', {'doc': . . ., 'count': 26776, 'lemmata': ''}),\n ('as well as', {'doc': as well as, 'count': 15420, 'lemmata': ''}),\n ('one of the', {'doc': one of the, 'count': 10677, 'lemmata': ''}),\n ('in order to', {'doc': in order to, 'count': 10076, 'lemmata': 'order'}),\n ('part of the', {'doc': part of the, 'count': 8928, 'lemmata': 'part'}),\n ('the end of', {'doc': the end of, 'count': 8427, 'lemmata': 'end'}),\n ('the New Testament',\n  {'doc': the New Testament, 'count': 8107, 'lemmata': 'New Testament'}),\n ('the fact that', {'doc': the fact that, 'count': 7347, 'lemmata': 'fact'}),\n ('that it is', {'doc': that it is, 'count': 5951, 'lemmata': ''}),\n ('according to the',\n  {'doc': according to the, 'count': 5886, 'lemmata': 'accord'}),\n ('there is no', {'doc': there is no, 'count': 5869, 'lemmata': 'be'}),\n ('the history of',\n  {'doc': the history of, 'count': 5862, 'lemmata': 'history'}),\n ('of the New', {'doc': of the New, 'count': 5785, 'lemmata': 'New'}),\n ('of the Holy', {'doc': of the Holy, 'count': 5737, 'lemmata': 'Holy'}),\n ('in which the', {'doc': in which the, 'count': 5699, 'lemmata': ''}),\n ('the use of', {'doc': the use of, 'count': 5601, 'lemmata': 'use'}),\n ('it is not', {'doc': it is not, 'count': 5598, 'lemmata': ''}),\n ('the Holy Spirit',\n  {'doc': the Holy Spirit, 'count': 5451, 'lemmata': 'Holy Spirit'}),\n ('of the Christian',\n  {'doc': of the Christian, 'count': 5441, 'lemmata': 'christian'}),\n ('is to be', {'doc': is to be, 'count': 5438, 'lemmata': ''})]"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngram_nlp_dict.items())[:20]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "'ark://27927/phx66812gq6'"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_ids =  list(ngramCount_dict.keys())\n",
    "article_ids[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def ngram_to_lemmata(ngram, ngram_nlp_dict):\n",
    "    try:\n",
    "        lemmata_sorted_str = ngram_nlp_dict[ngram][\"lemmata\"]\n",
    "        #print(\"found in preprocessed\") # (used for execution time tests...)\n",
    "    except:\n",
    "        try:\n",
    "            lemmata_sorted_str = doc_to_lemmata(nlp_light(ngram), postags)\n",
    "            #print(\"processed now\") # (used for execution time tests...)\n",
    "        except:\n",
    "            lemmata_sorted_str = \"\"\n",
    "    return lemmata_sorted_str"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.19 s, sys: 98.1 ms, total: 8.29 s\n",
      "Wall time: 8.31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_data = data_from_article_id(article_ids[10000])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "['c', 'd']"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\"a\", \"b\", \"c\", \"d\"][-2:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "article_ids =  list(ngramCount_dict.keys())\n",
    "step=100\n",
    "cleanedNgrams_tups = []\n",
    "gspread_cell_row = 1\n",
    "for num in range(0, len(article_ids), step):\n",
    "    actual_ids = article_ids[num:num+step]\n",
    "    with ThreadPoolExecutor(max_workers=step*1.5) as pool:\n",
    "        currently_parsed = list(pool.map(data_from_article_id,actual_ids))\n",
    "    cleanedNgrams_tups.extend(currently_parsed)\n",
    "    if num in range(0, len(article_ids), 500):\n",
    "        print(num)\n",
    "        mops_data.worksheet(\"trigrams_progress\").update_cell(gspread_cell_row, 1, str(num) + \" ({0})\".format(datetime.now().strftime(\"%H:%M:%S\")))\n",
    "        gspread_cell_row += 1\n",
    "        pickle.dump(cleanedNgrams_tups[-500:], open(\"../data/large_files/cleanedNgrams_tups_{0}.pickle\".format(str(num)), \"wb\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(\"../data/large_files/{0}Count_cleaned_dict.pickle\".format(ngram_type), \"wb\") as f:\n",
    "    pickle.dump(dict(cleanedNgrams_tups), f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Main application: bigrams"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<timed exec>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ngram_type = \"bigram\"\n",
    "ngramCount_dict = pickle.load(open(\"../data/large_files/{0}Count_dict.pickle\".format(ngram_type), \"rb\"))\n",
    "ngram_nlp_dict = pickle.load(open(\"../data/large_files/data_{0}s_nlp_dict.pickle\".format(ngram_type), \"rb\"))\n",
    "\n",
    "step=50\n",
    "cleanedNgrams_tups = []\n",
    "for num in range(0, len(article_ids), step):\n",
    "    actual_ids = article_ids[num:num+step]\n",
    "    with ThreadPoolExecutor(max_workers=step*1.5) as pool:\n",
    "        currently_parsed = list(pool.map(data_from_article_id,actual_ids))\n",
    "    cleanedNgrams_tups.extend(currently_parsed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "with open(\"../data/large_files/{0}Count_cleaned_dict.pickle\".format(ngram_type), \"wb\") as f:\n",
    "    pickle.dump(dict(cleanedNgrams_tups), f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}