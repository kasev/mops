{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "from collections import Counter\n",
    "import spacy\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "postags = [\"PROPN\", \"NOUN\", \"VERB\", \"ADJ\"]\n",
    "\n",
    "def clean_token(token):\n",
    "    token = re.sub(\"\\W\", \"\", token)\n",
    "    token = token[0] + token[1:].lower()\n",
    "    return token\n",
    "\n",
    "def doc_to_lemmata(doc, postags):\n",
    "    lemmata_list = [t.lemma_ for t in doc if t.pos_ in postags]\n",
    "    lemmata_list = [clean_token(el) for el in lemmata_list if len(el)>0]\n",
    "    joined_lemmata_sorted = \" \".join(sorted(lemmata_list))\n",
    "    return joined_lemmata_sorted\n",
    "\n",
    "def ngram_to_lemmata(ngram, ngram_nlp_dict):\n",
    "    try:\n",
    "        lemmata_sorted_str = doc_to_lemmata(ngram_nlp_dict[ngram][\"doc\"], postags)\n",
    "        #print(\"found in preprocessed\") # (used for execution time tests...)\n",
    "    except:\n",
    "        try:\n",
    "            lemmata_sorted_str = doc_to_lemmata(nlp(ngram), postags)\n",
    "            # print(\"processed now\") # (used for execution time tests...)\n",
    "        except:\n",
    "            lemmata_sorted_str = \"\"\n",
    "    return lemmata_sorted_str\n",
    "\n",
    "def article_data_to_lemmata(ngrams_dict, ngram_nlp_dict):\n",
    "    lemmata_tups = []\n",
    "    for string, count in ngrams_dict.items():\n",
    "        lemmata_tups.append((ngram_to_lemmata(string, ngram_nlp_dict), count))\n",
    "    #lemmata_tups = [tup for tup in lemmata_tups if len(tup[0].split()) > 1]\n",
    "    lemmata_dict = Counter()\n",
    "    for x,y in lemmata_tups:\n",
    "        lemmata_dict.update({x : y})\n",
    "    return lemmata_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "ngram_type = \"unigram\"\n",
    "ngramCount_dict = pickle.load(open(\"../data/large_files/{0}Count_dict.pickle\".format(ngram_type), \"rb\"))\n",
    "ngram_nlp_dict = pickle.load(open(\"../data/large_files/data_{0}s_nlp_dict.pickle\".format(ngram_type), \"rb\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 520 ms, sys: 7.57 ms, total: 527 ms\n",
      "Wall time: 551 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": "[('', 1460),\n ('have', 32),\n ('water', 26),\n ('cold', 21),\n ('hot', 20),\n ('city', 11),\n ('Paul', 10),\n ('church', 9),\n ('interpretation', 9),\n ('be', 9),\n ('word', 9),\n ('use', 9),\n ('lukewarm', 8),\n ('local', 8),\n ('Laodicea', 8),\n ('apply', 7),\n ('man', 7),\n ('mean', 7),\n ('spring', 7),\n ('allusion', 7),\n ('other', 6),\n ('certainty', 6),\n ('Christ', 6),\n ('Ramsay', 5),\n ('zesto', 5),\n ('mile', 5),\n ('letter', 5),\n ('fact', 5),\n ('see', 5),\n ('laodicean', 5),\n ('source', 5),\n ('person', 5),\n ('example', 5),\n ('do', 4),\n ('most', 4),\n ('christian', 4),\n ('passage', 4),\n ('supply', 4),\n ('heal', 4),\n ('M', 4),\n ('mineral', 4),\n ('certain', 4),\n ('usage', 4),\n ('purpose', 4),\n ('great', 4),\n ('circumstance', 3),\n ('know', 3),\n ('natural', 3),\n ('Ma', 3),\n ('course', 3),\n ('lukewarmness', 3),\n ('call', 3),\n ('hierapoli', 3),\n ('such', 3),\n ('spiritual', 3),\n ('stone', 3),\n ('sense', 3),\n ('normal', 3),\n ('form', 3),\n ('become', 3),\n ('text', 3),\n ('difficult', 3),\n ('common', 3),\n ('take', 3),\n ('traditional', 3),\n ('site', 3),\n ('neighbour', 3),\n ('difficulty', 3),\n ('same', 3),\n ('suggest', 3),\n ('term', 3),\n ('different', 3),\n ('white', 3),\n ('expect', 3),\n ('part', 3),\n ('exegetical', 3),\n ('imagery', 3),\n ('refer', 3),\n ('point', 3),\n ('greek', 3),\n ('refreshment', 3),\n ('much', 3),\n ('commendable', 3),\n ('kappaalphaiacgr', 3),\n ('read', 3),\n ('argue', 3),\n ('psychro', 3),\n ('condition', 3),\n ('door', 2),\n ('ineffective', 2),\n ('esse', 2),\n ('Romans', 2),\n ('ineffectiveness', 2),\n ('Denizli', 2),\n ('alternative', 2),\n ('derive', 2),\n ('iv', 2),\n ('weary', 2),\n ('series', 2),\n ('concerned', 2)]"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "test_ngrams_dict = (list(ngramCount_dict.items())[7000][1])\n",
    "test_output = article_data_to_lemmata(test_ngrams_dict, ngram_nlp_dict)\n",
    "sorted(test_output.items(), key=lambda pair: pair[1], reverse=True)[:100]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# comparing iteration vs comprehension"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 14s, sys: 2.95 s, total: 2min 17s\n",
      "Wall time: 2min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n = 100\n",
    "# small test...\n",
    "cleanedNgrams_tups = [(k, article_data_to_lemmata(v, ngram_nlp_dict)) for k, v in list(ngramCount_dict.items())[:n]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 9s, sys: 1.12 s, total: 2min 10s\n",
      "Wall time: 2min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# small test...\n",
    "cleanedNgrams_tups = [(k, article_data_to_lemmata(v, ngram_nlp_dict)) for k, v in list(ngramCount_dict.items())[:100]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "# using parallel computing..."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "def data_from_article_id(article_id):\n",
    "    article_data = article_data_to_lemmata(ngramCount_dict[article_id], ngram_nlp_dict)\n",
    "    return (article_id, article_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "'ark://27927/phx66812gq6'"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_ids =  list(ngramCount_dict.keys())\n",
    "article_ids[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "14103"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(article_ids)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 28s, sys: 1min, total: 3min 28s\n",
      "Wall time: 1min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "step=20\n",
    "cleanedNgrams_tups = []\n",
    "for num in range(0, 100, step):\n",
    "    actual_ids = article_ids[num:num+step]\n",
    "    with ThreadPoolExecutor(max_workers=step*1.5) as pool:\n",
    "        currently_parsed = list(pool.map(data_from_article_id,actual_ids))\n",
    "    cleanedNgrams_tups.extend(currently_parsed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Main application: unigrams"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10h 45min 58s, sys: 3h 53min 16s, total: 14h 39min 15s\n",
      "Wall time: 8h 35min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ngram_type = \"unigram\"\n",
    "ngramCount_dict = pickle.load(open(\"../data/large_files/{0}Count_dict.pickle\".format(ngram_type), \"rb\"))\n",
    "ngram_nlp_dict = pickle.load(open(\"../data/large_files/data_{0}s_nlp_dict.pickle\".format(ngram_type), \"rb\"))\n",
    "\n",
    "step=50\n",
    "cleanedNgrams_tups = []\n",
    "for num in range(0, len(article_ids), step):\n",
    "    actual_ids = article_ids[num:num+step]\n",
    "    with ThreadPoolExecutor(max_workers=step*1.5) as pool:\n",
    "        currently_parsed = list(pool.map(data_from_article_id,actual_ids))\n",
    "    cleanedNgrams_tups.extend(currently_parsed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "with open(\"../data/large_files/{0}Count_cleaned_dict.pickle\".format(ngram_type), \"wb\") as f:\n",
    "    pickle.dump(dict(cleanedNgrams_tups), f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Main application: trigrams"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<timed exec>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n",
      "\u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/concurrent/futures/_base.py\u001B[0m in \u001B[0;36mresult_iterator\u001B[0;34m()\u001B[0m\n\u001B[1;32m    607\u001B[0m                     \u001B[0;32mif\u001B[0m \u001B[0mtimeout\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 608\u001B[0;31m                         \u001B[0;32myield\u001B[0m \u001B[0mfs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mresult\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    609\u001B[0m                     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/concurrent/futures/_base.py\u001B[0m in \u001B[0;36mresult\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    439\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 440\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_condition\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwait\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    441\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/threading.py\u001B[0m in \u001B[0;36mwait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    311\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mtimeout\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 312\u001B[0;31m                 \u001B[0mwaiter\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0macquire\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    313\u001B[0m                 \u001B[0mgotit\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<timed exec>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n",
      "\u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/concurrent/futures/_base.py\u001B[0m in \u001B[0;36m__exit__\u001B[0;34m(self, exc_type, exc_val, exc_tb)\u001B[0m\n\u001B[1;32m    634\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    635\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__exit__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mexc_type\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mexc_val\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mexc_tb\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 636\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshutdown\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mwait\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    637\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    638\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/concurrent/futures/thread.py\u001B[0m in \u001B[0;36mshutdown\u001B[0;34m(self, wait, cancel_futures)\u001B[0m\n\u001B[1;32m    227\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mwait\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    228\u001B[0m             \u001B[0;32mfor\u001B[0m \u001B[0mt\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_threads\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 229\u001B[0;31m                 \u001B[0mt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    230\u001B[0m     \u001B[0mshutdown\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__doc__\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_base\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mExecutor\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshutdown\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__doc__\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/threading.py\u001B[0m in \u001B[0;36mjoin\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m   1051\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1052\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mtimeout\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1053\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_wait_for_tstate_lock\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1054\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1055\u001B[0m             \u001B[0;31m# the behavior of a negative timeout isn't documented, but\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/threading.py\u001B[0m in \u001B[0;36m_wait_for_tstate_lock\u001B[0;34m(self, block, timeout)\u001B[0m\n\u001B[1;32m   1067\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mlock\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# already determined that the C code is done\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1068\u001B[0m             \u001B[0;32massert\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_is_stopped\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1069\u001B[0;31m         \u001B[0;32melif\u001B[0m \u001B[0mlock\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0macquire\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mblock\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtimeout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1070\u001B[0m             \u001B[0mlock\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrelease\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1071\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_stop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ngram_type = \"trigram\"\n",
    "ngramCount_dict = pickle.load(open(\"../data/large_files/{0}Count_dict.pickle\".format(ngram_type), \"rb\"))\n",
    "ngram_nlp_dict = pickle.load(open(\"../data/large_files/data_{0}s_nlp_dict.pickle\".format(ngram_type), \"rb\"))\n",
    "\n",
    "step=50\n",
    "cleanedNgrams_tups = []\n",
    "for num in range(0, len(article_ids), step):\n",
    "    if num in range(0, len(article_ids), 1000):\n",
    "        print(num)\n",
    "    actual_ids = article_ids[num:num+step]\n",
    "    with ThreadPoolExecutor(max_workers=step*1.5) as pool:\n",
    "        currently_parsed = list(pool.map(data_from_article_id,actual_ids))\n",
    "    cleanedNgrams_tups.extend(currently_parsed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "with open(\"../data/large_files/{0}Count_cleaned_dict.pickle\".format(ngram_type), \"wb\") as f:\n",
    "    pickle.dump(dict(cleanedNgrams_tups), f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Main application: bigrams"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<timed exec>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ngram_type = \"bigram\"\n",
    "ngramCount_dict = pickle.load(open(\"../data/large_files/{0}Count_dict.pickle\".format(ngram_type), \"rb\"))\n",
    "ngram_nlp_dict = pickle.load(open(\"../data/large_files/data_{0}s_nlp_dict.pickle\".format(ngram_type), \"rb\"))\n",
    "\n",
    "step=50\n",
    "cleanedNgrams_tups = []\n",
    "for num in range(0, len(article_ids), step):\n",
    "    actual_ids = article_ids[num:num+step]\n",
    "    with ThreadPoolExecutor(max_workers=step*1.5) as pool:\n",
    "        currently_parsed = list(pool.map(data_from_article_id,actual_ids))\n",
    "    cleanedNgrams_tups.extend(currently_parsed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "with open(\"../data/large_files/{0}Count_cleaned_dict.pickle\".format(ngram_type), \"wb\") as f:\n",
    "    pickle.dump(dict(cleanedNgrams_tups), f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}