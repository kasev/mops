{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "from collections import Counter\n",
    "import spacy\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "postags = [\"PROPN\", \"NOUN\", \"VERB\", \"ADJ\"]\n",
    "\n",
    "def clean_token(token):\n",
    "    token = re.sub(\"\\W\", \"\", token)\n",
    "    token = token[0] + token[1:].lower()\n",
    "    return token\n",
    "\n",
    "def doc_to_lemmata(doc, postags):\n",
    "    lemmata_list = [t.lemma_ for t in doc if t.pos_ in postags]\n",
    "    lemmata_list = [clean_token(el) for el in lemmata_list if len(el)>0]\n",
    "    joined_lemmata_sorted = \" \".join(sorted(lemmata_list))\n",
    "    return joined_lemmata_sorted\n",
    "\n",
    "def ngram_to_lemmata(ngram, ngram_nlp_dict):\n",
    "    try:\n",
    "        lemmata_sorted_str = doc_to_lemmata(ngram_nlp_dict[ngram][\"doc\"], postags)\n",
    "        #print(\"found in preprocessed\") # (used for execution time tests...)\n",
    "    except:\n",
    "        try:\n",
    "            lemmata_sorted_str = doc_to_lemmata(nlp(ngram), postags)\n",
    "            # print(\"processed now\") # (used for execution time tests...)\n",
    "        except:\n",
    "            lemmata_sorted_str = \"\"\n",
    "    return lemmata_sorted_str\n",
    "\n",
    "def article_data_to_lemmata(ngrams_dict, ngram_nlp_dict):\n",
    "    lemmata_tups = []\n",
    "    for string, count in ngrams_dict.items():\n",
    "        lemmata_tups.append((ngram_to_lemmata(string, ngram_nlp_dict), count))\n",
    "    #lemmata_tups = [tup for tup in lemmata_tups if len(tup[0].split()) > 1]\n",
    "    lemmata_dict = Counter()\n",
    "    for x,y in lemmata_tups:\n",
    "        lemmata_dict.update({x : y})\n",
    "    return lemmata_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "ngram_type = \"unigram\"\n",
    "ngramCount_dict = pickle.load(open(\"../data/large_files/{0}Count_dict.pickle\".format(ngram_type), \"rb\"))\n",
    "ngram_nlp_dict = pickle.load(open(\"../data/large_files/data_{0}s_nlp_dict.pickle\".format(ngram_type), \"rb\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 520 ms, sys: 7.57 ms, total: 527 ms\n",
      "Wall time: 551 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": "[('', 1460),\n ('have', 32),\n ('water', 26),\n ('cold', 21),\n ('hot', 20),\n ('city', 11),\n ('Paul', 10),\n ('church', 9),\n ('interpretation', 9),\n ('be', 9),\n ('word', 9),\n ('use', 9),\n ('lukewarm', 8),\n ('local', 8),\n ('Laodicea', 8),\n ('apply', 7),\n ('man', 7),\n ('mean', 7),\n ('spring', 7),\n ('allusion', 7),\n ('other', 6),\n ('certainty', 6),\n ('Christ', 6),\n ('Ramsay', 5),\n ('zesto', 5),\n ('mile', 5),\n ('letter', 5),\n ('fact', 5),\n ('see', 5),\n ('laodicean', 5),\n ('source', 5),\n ('person', 5),\n ('example', 5),\n ('do', 4),\n ('most', 4),\n ('christian', 4),\n ('passage', 4),\n ('supply', 4),\n ('heal', 4),\n ('M', 4),\n ('mineral', 4),\n ('certain', 4),\n ('usage', 4),\n ('purpose', 4),\n ('great', 4),\n ('circumstance', 3),\n ('know', 3),\n ('natural', 3),\n ('Ma', 3),\n ('course', 3),\n ('lukewarmness', 3),\n ('call', 3),\n ('hierapoli', 3),\n ('such', 3),\n ('spiritual', 3),\n ('stone', 3),\n ('sense', 3),\n ('normal', 3),\n ('form', 3),\n ('become', 3),\n ('text', 3),\n ('difficult', 3),\n ('common', 3),\n ('take', 3),\n ('traditional', 3),\n ('site', 3),\n ('neighbour', 3),\n ('difficulty', 3),\n ('same', 3),\n ('suggest', 3),\n ('term', 3),\n ('different', 3),\n ('white', 3),\n ('expect', 3),\n ('part', 3),\n ('exegetical', 3),\n ('imagery', 3),\n ('refer', 3),\n ('point', 3),\n ('greek', 3),\n ('refreshment', 3),\n ('much', 3),\n ('commendable', 3),\n ('kappaalphaiacgr', 3),\n ('read', 3),\n ('argue', 3),\n ('psychro', 3),\n ('condition', 3),\n ('door', 2),\n ('ineffective', 2),\n ('esse', 2),\n ('Romans', 2),\n ('ineffectiveness', 2),\n ('Denizli', 2),\n ('alternative', 2),\n ('derive', 2),\n ('iv', 2),\n ('weary', 2),\n ('series', 2),\n ('concerned', 2)]"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "test_ngrams_dict = (list(ngramCount_dict.items())[7000][1])\n",
    "test_output = article_data_to_lemmata(test_ngrams_dict, ngram_nlp_dict)\n",
    "sorted(test_output.items(), key=lambda pair: pair[1], reverse=True)[:100]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# comparing iteration vs comprehension"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 14s, sys: 2.95 s, total: 2min 17s\n",
      "Wall time: 2min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n = 100\n",
    "# small test...\n",
    "cleanedNgrams_tups = [(k, article_data_to_lemmata(v, ngram_nlp_dict)) for k, v in list(ngramCount_dict.items())[:n]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 9s, sys: 1.12 s, total: 2min 10s\n",
      "Wall time: 2min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# small test...\n",
    "cleanedNgrams_tups = [(k, article_data_to_lemmata(v, ngram_nlp_dict)) for k, v in list(ngramCount_dict.items())[:100]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "# using parallel computing..."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "def data_from_article_id(article_id):\n",
    "    article_data = article_data_to_lemmata(ngramCount_dict[article_id], ngram_nlp_dict)\n",
    "    return (article_id, article_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "'ark://27927/phx66812gq6'"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_ids =  list(ngramCount_dict.keys())\n",
    "article_ids[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "14103"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(article_ids)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 28s, sys: 1min, total: 3min 28s\n",
      "Wall time: 1min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "step=20\n",
    "cleanedNgrams_tups = []\n",
    "for num in range(0, 100, step):\n",
    "    actual_ids = article_ids[num:num+step]\n",
    "    with ThreadPoolExecutor(max_workers=step*1.5) as pool:\n",
    "        currently_parsed = list(pool.map(data_from_article_id,actual_ids))\n",
    "    cleanedNgrams_tups.extend(currently_parsed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Main application: unigrams"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ngram_type = \"unigram\"\n",
    "ngramCount_dict = pickle.load(open(\"../data/large_files/{0}Count_dict.pickle\".format(ngram_type), \"rb\"))\n",
    "ngram_nlp_dict = pickle.load(open(\"../data/large_files/data_{0}s_nlp_dict.pickle\".format(ngram_type), \"rb\"))\n",
    "\n",
    "step=50\n",
    "cleanedNgrams_tups = []\n",
    "for num in range(0, len(article_ids), step):\n",
    "    actual_ids = article_ids[num:num+step]\n",
    "    with ThreadPoolExecutor(max_workers=step*1.5) as pool:\n",
    "        currently_parsed = list(pool.map(data_from_article_id,actual_ids))\n",
    "    cleanedNgrams_tups.extend(currently_parsed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(\"../data/large_files/{0}Count_cleaned_dict.pickle\".format(ngram_type), \"wb\") as f:\n",
    "    pickle.dump(dict(cleanedNgrams_tups), f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Main application: trigrams"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ngram_type = \"trigram\"\n",
    "ngramCount_dict = pickle.load(open(\"../data/large_files/{0}Count_dict.pickle\".format(ngram_type), \"rb\"))\n",
    "ngram_nlp_dict = pickle.load(open(\"../data/large_files/data_{0}s_nlp_dict.pickle\".format(ngram_type), \"rb\"))\n",
    "\n",
    "step=50\n",
    "cleanedNgrams_tups = []\n",
    "for num in range(0, len(article_ids), step):\n",
    "    if num in range(0, len(article_ids), 1000):\n",
    "        print(num)\n",
    "    actual_ids = article_ids[num:num+step]\n",
    "    with ThreadPoolExecutor(max_workers=step*1.5) as pool:\n",
    "        currently_parsed = list(pool.map(data_from_article_id,actual_ids))\n",
    "    cleanedNgrams_tups.extend(currently_parsed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(\"../data/large_files/{0}Count_cleaned_dict.pickle\".format(ngram_type), \"wb\") as f:\n",
    "    pickle.dump(dict(cleanedNgrams_tups), f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Main application: bigrams"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ngram_type = \"bigram\"\n",
    "ngramCount_dict = pickle.load(open(\"../data/large_files/{0}Count_dict.pickle\".format(ngram_type), \"rb\"))\n",
    "ngram_nlp_dict = pickle.load(open(\"../data/large_files/data_{0}s_nlp_dict.pickle\".format(ngram_type), \"rb\"))\n",
    "\n",
    "step=50\n",
    "cleanedNgrams_tups = []\n",
    "for num in range(0, len(article_ids), step):\n",
    "    actual_ids = article_ids[num:num+step]\n",
    "    with ThreadPoolExecutor(max_workers=step*1.5) as pool:\n",
    "        currently_parsed = list(pool.map(data_from_article_id,actual_ids))\n",
    "    cleanedNgrams_tups.extend(currently_parsed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(\"../data/large_files/{0}Count_cleaned_dict.pickle\".format(ngram_type), \"wb\") as f:\n",
    "    pickle.dump(dict(cleanedNgrams_tups), f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}